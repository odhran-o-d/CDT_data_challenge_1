{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm.auto import tqdm\n",
    "import utils\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Grab a GPU if there is one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device: 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using {} device: {}\".format(device, torch.cuda.current_device()))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of capture24.npz: ['X_feats', 'y', 'pid', 'time', 'annotation']\n",
      "X_feats shape: (330610, 125)\n",
      "y shape: (330610,)\n",
      "pid shape: (330610,)\n",
      "time shape: (330610,)\n",
      "annotation shape: (330610,)\n",
      "X_raw shape: (330610, 3, 3000)\n",
      "\n",
      "Contents of capture24_test.npz: ['X_feats', 'pid', 'time']\n"
     ]
    }
   ],
   "source": [
    "data = np.load('capture24.npz', allow_pickle=True)\n",
    "print(\"Contents of capture24.npz:\", data.files)\n",
    "X_feats, y, pid, time, annotation = \\\n",
    "    data['X_feats'], data['y'], data['pid'], data['time'], data['annotation']\n",
    "print('X_feats shape:', X_feats.shape)\n",
    "print('y shape:', y.shape)\n",
    "print('pid shape:', pid.shape)\n",
    "print('time shape:', time.shape)\n",
    "print('annotation shape:', annotation.shape)\n",
    "X_raw = np.load('X_raw.npy', mmap_mode='r')\n",
    "print('X_raw shape:', X_raw.shape)\n",
    "data_unl = np.load('capture24_test.npz')\n",
    "print(\"\\nContents of capture24_test.npz:\", data_unl.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval2seq(mydata):\n",
    "    X_feats, y, pid, time, annotation = \\\n",
    "    data['X_feats'], data['y'], data['pid'], data['time'], data['annotation']\n",
    "    \n",
    "    # Get all the unique pids \n",
    "    subjects = np.unique(data['pid'])\n",
    "    X_tr = []\n",
    "    y_tr =  []\n",
    "    pid_tr =  []\n",
    "    for subject in subjects:\n",
    "        X_tr.append(X_raw[pid==subject])\n",
    "        y_tr.append(y[pid==subject])\n",
    "        pid_tr.append(subject)\n",
    "    x = np.array(X_tr)\n",
    "    y = np.array(y_tr)\n",
    "    pid = np.array(pid_tr)\n",
    "    \n",
    "    return x, y, pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, pid = interval2seq(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the train&test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (137,)\n",
      "Shape of X_test (2,)\n"
     ]
    }
   ],
   "source": [
    "# Hold out some participants for testing the model\n",
    "pids_test = [2, 3]\n",
    "mask_test = np.isin(pid, pids_test)\n",
    "mask_train = ~mask_test\n",
    "y_train, y_test = y[mask_train], y[mask_test]\n",
    "pid_train, pid_test = pid[mask_train], pid[mask_test]\n",
    "# X[mask_train] and X[mask_test] if you like to live dangerously\n",
    "X_train = utils.ArrayFromMask(x, mask_train)\n",
    "X_test = utils.ArrayFromMask(x, mask_test)\n",
    "print(\"Shape of X_train\", X_train.shape)\n",
    "print(\"Shape of X_test\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(977, 3, 3000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2051, 3, 3000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    ''' Convolution + batch normalization + ReLU is a common trio '''\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels,\n",
    "        kernel_size=5, stride=1, padding=1, bias=True\n",
    "    ):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels,\n",
    "                kernel_size, stride, padding, bias=bias),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    ''' Typical CNN design with pyramid-like structure '''\n",
    "    def __init__(self, output_size=5, in_channels=3, num_filters_init=8):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            ConvBNReLU(in_channels, num_filters_init,\n",
    "            8, 4, 2, bias=False),  # 1500 -> 750\n",
    "            ConvBNReLU(num_filters_init, num_filters_init*2,\n",
    "            6, 4, 2, bias=False),  # 750 -> 188\n",
    "            ConvBNReLU(num_filters_init*2, num_filters_init*4,\n",
    "            8, 4, 2, bias=False),  # 188 -> 47\n",
    "            ConvBNReLU(num_filters_init*4, num_filters_init*8,\n",
    "            3, 2, 1, bias=False),  # 47 -> 24\n",
    "            ConvBNReLU(num_filters_init*8, num_filters_init*16,\n",
    "            4, 2, 1, bias=False),  # 24 -> 12\n",
    "            ConvBNReLU(num_filters_init*16, num_filters_init*32,\n",
    "            4, 2, 1, bias=False),  # 12 -> 6\n",
    "            ConvBNReLU(num_filters_init*32, num_filters_init*64,\n",
    "            6, 1, 0, bias=False),  # 6 -> 1\n",
    "            nn.Conv1d(num_filters_init*64, output_size,\n",
    "            1, 1, 0, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x).view(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    ''' Typical CNN design with pyramid-like structure '''\n",
    "    def __init__(self, output_size=128, in_channels=3, num_filters_init=8):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            ConvBNReLU(in_channels, 64),\n",
    "            ConvBNReLU(64, 64),\n",
    "            ConvBNReLU(64, 64),\n",
    "            ConvBNReLU(64, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x).view(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combine(nn.Module):\n",
    "    def __init__(self, num_hn=128, num_class=5):\n",
    "        super(Combine, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        cnnoutputsize = 234\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=cnnoutputsize, \n",
    "            hidden_size=num_hn, \n",
    "            batch_first=True)\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=num_hn, \n",
    "            hidden_size=num_hn, \n",
    "            batch_first=True)\n",
    "        self.linear = nn.Linear(num_hn, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        \n",
    "        r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_out, (h_n, h_c) = self.rnn1(r_in)\n",
    "        r_out, (h_n, h_c) = self.rnn1(r_in)\n",
    "\n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        return F.log_softmax(r_out2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine(\n",
      "  (cnn): CNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): ConvBNReLU(\n",
      "        (main): Sequential(\n",
      "          (0): Conv1d(3, 64, kernel_size=(5,), stride=(1,), padding=(1,))\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ConvBNReLU(\n",
      "        (main): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(1,))\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ConvBNReLU(\n",
      "        (main): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(1,))\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): ConvBNReLU(\n",
      "        (main): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(1,))\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (rnn1): LSTM(128, 128, batch_first=True)\n",
      "  (linear): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_filters_init = 8  # initial num of filters -- see class definition\n",
    "in_channels = 3  # num channels of the signal -- equal to 3 for our raw triaxial timeseries\n",
    "output_size = 5  # number of classes (sleep, sedentary, etc...)\n",
    "num_epoch = 5  # num epochs (full loops though the training set) for SGD training\n",
    "lr = 1e-3  # learning rate in SGD\n",
    "batch_size = 32  # size of the mini-batch in SGD\n",
    "\n",
    "#cnn = CNN(\n",
    "#    output_size=output_size,\n",
    "#    in_channels=in_channels,\n",
    "#).to(device)\n",
    "\n",
    "cnnlstm = Combine().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnnlstm.parameters(), lr=lr, amsgrad=True)\n",
    "print(cnnlstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        print(nn)\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed the data into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(X, y=None, batch_size=1, shuffle=False):\n",
    "    ''' Create a (batch) iterator over the dataset. Alternatively, use PyTorch's\n",
    "    Dataset and DataLoader classes -- See\n",
    "    https://pytorch.org/tutorials/beginner/data_loading_tutorial.html '''\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(np.arange(len(X)))\n",
    "    else:\n",
    "        idxs = np.arange(len(X))\n",
    "    for i in range(0, len(idxs), batch_size):\n",
    "        idxs_batch = idxs[i:i+batch_size]\n",
    "        X_batch = X[idxs_batch]\n",
    "        print(X_batch.shape)\n",
    "        X_batch = torch.from_numpy(X_batch)\n",
    "        if y is None:\n",
    "            yield X_batch\n",
    "        else:\n",
    "            y_batch = y[idxs_batch]\n",
    "            y_batch = torch.from_numpy(y_batch)\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "\n",
    "def forward_by_batches(cnn, X):\n",
    "    ''' Forward pass model on a dataset. Do this by batches so that we do\n",
    "    not blow up the memory. '''\n",
    "    Y = []\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for x in create_dataloader(X, batch_size=1024, shuffle=False):  # do not shuffle here!\n",
    "            x = x.to(device)\n",
    "            Y.append(cnn(x))\n",
    "    cnn.train()\n",
    "    Y = torch.cat(Y)\n",
    "    return Y\n",
    "\n",
    "def evaluate_model(cnn, prior, emission, transition, X, y, pid=None):\n",
    "    Y_pred = forward_by_batches(cnn, X)  # scores\n",
    "    loss = F.cross_entropy(Y_pred, torch.from_numpy(y).to(device)).item()\n",
    "    Y_pred = F.softmax(Y_pred, dim=1)  # convert to probabilities\n",
    "    y_pred = torch.argmax(Y_pred, dim=1)  # convert to classes\n",
    "    y_pred = y_pred.cpu().numpy()  # cast to numpy array\n",
    "    y_pred = utils.viterbi(y_pred, prior, transition, emission)  # HMM smoothing\n",
    "    scores = utils.compute_scores(y, y_pred)\n",
    "    return loss, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4a0bfef60e4ff1afa295c44e7d93f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8f2da155dbbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-bfec34763214>\u001b[0m in \u001b[0;36mcreate_dataloader\u001b[0;34m(X, y, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "accuracy_history = []\n",
    "balanced_accuracy_history = []\n",
    "kappa_history = []\n",
    "loss_history = []\n",
    "loss_history_train = []\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    dataloader = create_dataloader(X_train, y_train, batch_size, shuffle=True)\n",
    "    losses = []\n",
    "    for x, target in dataloader:\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        cnn.zero_grad()\n",
    "        output = cnnlstm(x)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging -- track train loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Evaluate performance at the end of each epoch (full loop through the\n",
    "    # training set). We could also do this at every iteration, but this would\n",
    "    # be very expensive since we are evaluating on a large dataset.\n",
    "    # Aditionally, at the end of each epoch we train a Hidden Markov Model to\n",
    "    # smooth the predictions of the CNN.\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Logging -- average train loss in this epoch\n",
    "    loss_history_train.append(np.mean(losses))\n",
    "\n",
    "    # Compute HMM params\n",
    "    prior, emission, transition = train_hmm(cnn, X_train, y_train)\n",
    "\n",
    "    # Logging -- evalutate performance on test set\n",
    "    loss_test, scores_test = evaluate_model(\n",
    "        cnn, prior, emission, transition, X_test, y_test, pid_test\n",
    "    )\n",
    "    loss_history.append(loss_test)\n",
    "    accuracy_history.append(scores_test['accuracy'])\n",
    "    balanced_accuracy_history.append(scores_test['balanced_accuracy'])\n",
    "    kappa_history.append(scores_test['kappa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
